{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../..\")\n",
    "\n",
    "from utils import llm_api\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "from tasks.graph import graph_utils\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import bipartite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config for running the Relevance Estimation algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": \"llama3.1:8b\",\n",
    "    \"god_model\": \"gpt-3.5-turbo\",\n",
    "    \"graph\": {\n",
    "        \"samples\": 5,\n",
    "        \"nodes\": 30,\n",
    "        \"edges\": 200,\n",
    "        \"vertex_step_size\": 5\n",
    "    },\n",
    "    \"algorithms\": {\n",
    "        \"naive\": {\n",
    "            \"chunk_size\": 4\n",
    "        },\n",
    "        \"bi_graph\": {\n",
    "            \"shuffles\": 4,\n",
    "            \"batch_size\": 8\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the prompt given the task (I, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(edges, vertex):\n",
    "    prompt = \"Consider an undirected graph with the following edges as pairs of source and target nodes: \\n\"\n",
    "\n",
    "    for e in edges:\n",
    "        prompt += str(e) + \"\\n\"\n",
    "    # prompt += str(edges)\n",
    "\n",
    "    prompt += f\"What is the degree of node {vertex}? Answer with just a number without further explanation.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bipartite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part implements the bipartite evaluation method. Most of other implementations are in 'utils' dirs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_for_scores(query, input_batch):\n",
    "    prompt = \"Consider a graph with the following list of edges:\\n\"\n",
    "    for b in input_batch:\n",
    "        prompt += f\"- {b}\\n\"\n",
    "    prompt += f\"\\nGive a list of comma-separated numbers as the relevance scores of each edge to the query '{query}'. The list should have the same number of elements as the edge list. Each number in the list corresponds to how much the edge is relevant to the query. Each number is an integer between 0 and 10. Answer in one line containing the comma separated scores.\"\n",
    "    return prompt\n",
    "\n",
    "def ask_score(input_batch, query):\n",
    "    prompt = build_prompt_for_scores(query, input_batch)\n",
    "    answer = llm_api.ask([prompt], config[\"model\"])\n",
    "    p1 = r\"(\\s*\\d+\\s*)\"\n",
    "    p2 = r\"(,(\\s*\\d+\\s*))+\"\n",
    "    pattern = \"(\" + p1 + (len(input_batch) - 1) * p2 + \")\"\n",
    "    for line in answer.split(\"\\n\"):\n",
    "        if len(re.findall(pattern, line)) > 0:\n",
    "            answer = re.findall(pattern, line)[0][0]\n",
    "            print(\"[BI_GRAPH]\", line, input_batch, query)\n",
    "            break\n",
    "    return [llm_api.take_out_number(a) for a in answer.split(\",\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the bipartite method given the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = config[\"algorithms\"][\"bi_graph\"][\"shuffles\"]\n",
    "m = config[\"algorithms\"][\"bi_graph\"][\"batch_size\"]\n",
    "\n",
    "def run_bi_graph(prompt_graph, result_dict, node_size, step_size):\n",
    "    prompt = graph_utils.get_list_of_edges(prompt_graph)\n",
    "    random.shuffle(prompt) # Random Shuffle the list of edges\n",
    "\n",
    "    for j in np.arange(node_size, step=step_size):        \n",
    "        time.sleep(0.5)\n",
    "\n",
    "        query = f\"What is the degree of node {j}?\"\n",
    "        g, element_nodes = bipartite.create_bi_graph(k=k, m=m, prompt=prompt, query=query, ask_score=ask_score)\n",
    "        bipartite.learn_bi_graph(g)\n",
    "        new_prompt = sorted(prompt, key=lambda x: element_nodes[x].payload, reverse=True)\n",
    "\n",
    "        result_dict[\"vertex\"].append(j)\n",
    "        result_dict[\"original_prompt\"].append(prompt)\n",
    "        result_dict[\"generated_prompt\"].append(new_prompt)\n",
    "        result_dict[\"algorithm\"].append(\"bi_graph\")\n",
    "        result_dict[\"model\"].append(config[\"model\"])\n",
    "        result_dict[\"edge_size\"].append(config[\"graph\"][\"edges\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the warm-up algorithm implementation which is comparably easier than the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_edges(graph, question, model=\"gpt-3.5-turbo\"):\n",
    "    ans = llm_api.ask(f\"Consider an undirected graph with the following edges as pairs of source and target vertecies: \\n{graph}\\n Which edges are directly related to the query '{question}'? Just give a list of tuples with no furthur explanation.\", model)\n",
    "    return ans\n",
    "\n",
    "# chunk_size = 4\n",
    "chunk_size = config[\"algorithms\"][\"naive\"][\"chunk_size\"]\n",
    "\n",
    "def run_naive(g, result_dict, node_size, step_size):\n",
    "    edges = graph_utils.get_list_of_edges(g)\n",
    "    random.shuffle(edges)\n",
    "    chs = []\n",
    "    for h in range(len(edges) // chunk_size):\n",
    "        chs.append(edges[h * chunk_size: (h + 1) * chunk_size])\n",
    "\n",
    "    for j in np.arange(node_size, step=step_size):\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        question = f\"What is the degree of vertex {j}?\"\n",
    "        reg = r'(\\(\\d+,( |)\\d+\\))'\n",
    "\n",
    "        related = []\n",
    "        not_related = []\n",
    "\n",
    "        for ch in chs:\n",
    "            time.sleep(1)\n",
    "            result = get_related_edges(ch, question, config[\"model\"])\n",
    "            print(\"[Naive] related edges: \", result)\n",
    "            eds = [eval(x[0]) for x in re.findall(reg, result)]\n",
    "            not_eds = list(filter(lambda x: x not in eds, ch))\n",
    "            related.extend(eds)\n",
    "            not_related.extend(not_eds)\n",
    "\n",
    "        final_edges = related + not_related\n",
    "        final_edges = final_edges[:len(edges)]\n",
    "\n",
    "\n",
    "        result_dict[\"vertex\"].append(j)\n",
    "        result_dict[\"original_prompt\"].append(edges)\n",
    "        result_dict[\"generated_prompt\"].append(related + not_related)\n",
    "        result_dict[\"algorithm\"].append(\"warmup\")\n",
    "        result_dict[\"model\"].append(config[\"model\"])\n",
    "        result_dict[\"edge_size\"].append(config[\"graph\"][\"edges\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opt & Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing the upper and lower bounds for each given task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_edges(edges, vertex):\n",
    "    rels = list(filter(lambda x: vertex in x, edges))\n",
    "    non_rels = list(filter(lambda x: vertex not in x, edges))\n",
    "    return rels + non_rels\n",
    "\n",
    "def find_optimal_prompt(g, result_dict, node_size, step_size):\n",
    "    edges = graph_utils.get_list_of_edges(g)\n",
    "    for j in np.arange(node_size, step=step_size):\n",
    "        sorted_edges = sort_edges(edges, j)\n",
    "        \n",
    "        result_dict[\"vertex\"].append(j)\n",
    "        result_dict[\"original_prompt\"].append(edges)\n",
    "        result_dict[\"generated_prompt\"].append(sorted_edges)\n",
    "        result_dict[\"algorithm\"].append(\"opt\")\n",
    "        result_dict[\"model\"].append(config[\"model\"])\n",
    "        result_dict[\"edge_size\"].append(config[\"graph\"][\"edges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random(g, result_dict, node_size, step_size):\n",
    "    edges = graph_utils.get_list_of_edges(g)\n",
    "    random.shuffle(edges)\n",
    "    for j in np.arange(node_size, step=step_size):\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        result_dict[\"vertex\"].append(j)\n",
    "        result_dict[\"original_prompt\"].append(edges)\n",
    "        result_dict[\"generated_prompt\"].append(edges)\n",
    "        result_dict[\"algorithm\"].append(\"random\")\n",
    "        result_dict[\"model\"].append(config[\"model\"])\n",
    "        result_dict[\"edge_size\"].append(config[\"graph\"][\"edges\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
